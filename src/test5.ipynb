{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39eee6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on a GPU MIG instance\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.facecolor'] = '#1F1F1F'\n",
    "plt.rcParams['axes.facecolor'] = '#1F1F1F'\n",
    "plt.rcParams['savefig.facecolor'] = '#1F1F1F'\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "def check_mig_with_smi():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"-L\"], text=True)\n",
    "        return \"MIG\" in output\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "if check_mig_with_smi():\n",
    "    print(\"Running on a GPU MIG instance\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "else:\n",
    "    print(\"Not running on a GPU MIG instance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59e7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import einsum, nn\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, dim_out, 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, dim_out, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2),\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        time_emb = self.mlp(time_emb)\n",
    "        time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "        scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = dim\n",
    "        self.init_conv = nn.Conv2d(\n",
    "            input_channels, init_dim, 1, padding=0\n",
    "        )  # changed to 1 and 0 from 7,3\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        (\n",
    "                            Downsample(dim_in, dim_out)\n",
    "                            if not is_last\n",
    "                            else nn.Conv2d(dim_in, dim_out, 3, padding=1)\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        (\n",
    "                            Upsample(dim_out, dim_in)\n",
    "                            if not is_last\n",
    "                            else nn.Conv2d(dim_out, dim_in, 3, padding=1)\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = channels\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1196e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    107\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m    108\u001b[39m model.train()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/grad312/lib/python3.12/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n\u001b[32m   1753\u001b[39m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1756\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from scipy import integrate\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "eps = 0.001\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Unet(\n",
    "    dim=32,\n",
    "    channels=1,\n",
    "    dim_mults=(1, 2, 4),\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def euler_sampler(model, shape, sample_N):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z0 = torch.randn(shape, device=device)\n",
    "        x = z0.detach().clone()\n",
    "\n",
    "        dt = 1.0 / sample_N\n",
    "        for i in range(sample_N):\n",
    "            num_t = i / sample_N * (1 - eps) + eps\n",
    "            t = torch.ones(shape[0], device=device) * num_t\n",
    "            pred = model(x, t * 999)\n",
    "\n",
    "            x = x.detach().clone() + pred * dt\n",
    "\n",
    "        nfe = sample_N\n",
    "        return x.cpu(), nfe\n",
    "\n",
    "\n",
    "def to_flattened_numpy(x):\n",
    "    return x.detach().cpu().numpy().reshape((-1,))\n",
    "\n",
    "\n",
    "def from_flattened_numpy(x, shape):\n",
    "    return torch.from_numpy(x.reshape(shape))\n",
    "\n",
    "\n",
    "def rk45_sampler(model, shape):\n",
    "\n",
    "    rtol = atol = 1e-05\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z0 = torch.randn(shape, device=device)\n",
    "        x = z0.detach().clone()\n",
    "\n",
    "        def ode_func(t, x):\n",
    "            x = from_flattened_numpy(x, shape).to(device).type(torch.float32)\n",
    "            vec_t = torch.ones(shape[0], device=x.device) * t\n",
    "            drift = model(x, vec_t * 999)\n",
    "\n",
    "            return to_flattened_numpy(drift)\n",
    "\n",
    "        solution = integrate.solve_ivp(\n",
    "            ode_func,\n",
    "            (eps, 1),\n",
    "            to_flattened_numpy(x),\n",
    "            rtol=rtol,\n",
    "            atol=atol,\n",
    "            method=\"RK45\",\n",
    "        )\n",
    "        nfe = solution.nfev\n",
    "        x = torch.tensor(solution.y[:, -1]).reshape(shape).type(torch.float32)\n",
    "\n",
    "        return x, nfe\n",
    "\n",
    "\n",
    "def imshow(img, filename):\n",
    "    img = img * 0.3081 + 0.1307\n",
    "    img = np.clip(img, 0, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(npimg[0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filename, bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "\n",
    "def save_img_grid(img, filename):\n",
    "    img_grid = torchvision.utils.make_grid(img, nrow=10)\n",
    "    imshow(img_grid, filename)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch, _ in dataloader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z0 = torch.randn_like(batch)\n",
    "        t = torch.rand(batch.shape[0], device=device) * (1 - eps) + eps\n",
    "\n",
    "        t_expand = t.view(-1, 1, 1, 1).repeat(\n",
    "            1, batch.shape[1], batch.shape[2], batch.shape[3]\n",
    "        )\n",
    "        #print(t_expand.shape)\n",
    "        perturbed_data = t_expand * batch + (1 - t_expand) * z0\n",
    "        target = batch - z0\n",
    "\n",
    "        score = model(perturbed_data, t * 999)\n",
    "\n",
    "        losses = torch.square(score - target)\n",
    "        losses = torch.mean(losses.reshape(losses.shape[0], -1), dim=-1)\n",
    "\n",
    "        loss = torch.mean(losses)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "    images, nfe = euler_sampler(model, shape=(100, 1, 28, 28), sample_N=1)\n",
    "    save_img_grid(images, f\"data/euler_epoch_{epoch + 1}_nfe_{nfe}.png\")\n",
    "\n",
    "    images, nfe = euler_sampler(model, shape=(100, 1, 28, 28), sample_N=2)\n",
    "    save_img_grid(images, f\"data/euler_epoch_{epoch + 1}_nfe_{nfe}.png\")\n",
    "\n",
    "    images, nfe = euler_sampler(model, shape=(100, 1, 28, 28), sample_N=10)\n",
    "    save_img_grid(images, f\"data/euler_epoch_{epoch + 1}_nfe_{nfe}.png\")\n",
    "\n",
    "    images, nfe = rk45_sampler(model, shape=(100, 1, 28, 28))\n",
    "    save_img_grid(images, f\"data/rk45_epoch_{epoch + 1}_nfe_{nfe}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0f82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
