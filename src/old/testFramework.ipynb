{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.4.1+cu121\n",
      "Cuda available:  True\n",
      "Cuda Version:  12.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, ConcatDataset, TensorDataset, DataLoader, Dataset, Subset\n",
    "from matplotlib import animation\n",
    "import os\n",
    "import wandb\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Cuda available: \",torch.cuda.is_available())\n",
    "print(\"Cuda Version: \",torch.version.cuda)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#from dataloaders.SimpleLoaderBubbleML import *\n",
    "from modelComp.UNet import *\n",
    "from trainers.simpletrainer import one_epoch_train_simple, one_epoch_val_simple\n",
    "from dataloaders.SimpleLoaderBubbleML import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the HDF5 file:\n",
      "dfun\n",
      "int-runtime-params\n",
      "pressure\n",
      "real-runtime-params\n",
      "temperature\n",
      "velx\n",
      "vely\n",
      "x\n",
      "y\n",
      "\n",
      "Metadata size: (109,)\n",
      "Key type: <class 'numpy.bytes_'>\n",
      "Val type: <class 'numpy.float64'>\n",
      "\n",
      "Reynolds Number: 238.0952380952381\n",
      "\n",
      "checkpointfileintervaltime: 5.0\n",
      "checkpointfileintervalz: 1.7976931348623157e+308\n",
      "derefine_cutoff_1: 0.2\n",
      "derefine_cutoff_2: 0.2\n",
      "derefine_cutoff_3: 0.2\n",
      "derefine_cutoff_4: 0.2\n",
      "dr_dtmincontinue: 0.0\n",
      "dr_tstepslowstartfactor: 0.1\n",
      "dtinit: 0.0001\n",
      "dtmax: 0.0004\n",
      "dtmin: 1e-10\n",
      "gr_amrexmg_tol: 1e-13\n",
      "gr_btworkchildscaling: 1.0\n",
      "gr_btworkdefaultleaf: 2.0\n",
      "gr_btworkdefaultpar: 1.0\n",
      "gr_btworklbleaf: 0.0\n",
      "gr_btworklbpar: 0.0\n",
      "gr_btworkubleaf: 1000000.0\n",
      "gr_btworkubpar: 1000000.0\n",
      "gr_lrefinemaxredlogbase: 10.0\n",
      "gr_lrefinemaxredradiusfact: 0.0\n",
      "gr_lrefinemaxredtref: 0.0\n",
      "gr_lrefinemaxredtimescale: 1.0\n",
      "gr_lrefmaxtime_1: -1.0\n",
      "gr_lrefmaxtime_10: -1.0\n",
      "gr_lrefmaxtime_11: -1.0\n",
      "gr_lrefmaxtime_12: -1.0\n",
      "gr_lrefmaxtime_13: -1.0\n",
      "gr_lrefmaxtime_14: -1.0\n",
      "gr_lrefmaxtime_15: -1.0\n",
      "gr_lrefmaxtime_16: -1.0\n",
      "gr_lrefmaxtime_17: -1.0\n",
      "gr_lrefmaxtime_18: -1.0\n",
      "gr_lrefmaxtime_19: -1.0\n",
      "gr_lrefmaxtime_2: -1.0\n",
      "gr_lrefmaxtime_20: -1.0\n",
      "gr_lrefmaxtime_3: -1.0\n",
      "gr_lrefmaxtime_4: -1.0\n",
      "gr_lrefmaxtime_5: -1.0\n",
      "gr_lrefmaxtime_6: -1.0\n",
      "gr_lrefmaxtime_7: -1.0\n",
      "gr_lrefmaxtime_8: -1.0\n",
      "gr_lrefmaxtime_9: -1.0\n",
      "ht_nusselt: 1.0\n",
      "ht_prandtl: 8.4\n",
      "ht_tbulk: 0.0\n",
      "ht_twall_high: 1.0\n",
      "ht_twall_low: 0.0\n",
      "ins_cfl: 0.05\n",
      "ins_dpdx: 0.0\n",
      "ins_dpdy: 0.0\n",
      "ins_dpdz: 0.0\n",
      "ins_dtspec: 0.0001\n",
      "ins_gravx: 0.0\n",
      "ins_gravy: -1.0\n",
      "ins_gravz: 0.0\n",
      "ins_inflowvelscale: 1.0\n",
      "ins_invreynolds: 0.0042\n",
      "ins_sigma: 0.05\n",
      "mph_cpgas: 0.83\n",
      "mph_stefan: 0.6642\n",
      "mph_tsat: 0.16\n",
      "mph_ipropsmear: 1.0\n",
      "mph_invweber: 1.0\n",
      "mph_mugas: 1.0\n",
      "mph_rhogas: 0.0083\n",
      "mph_thcogas: 0.25\n",
      "nusts: 0.1\n",
      "plotfileintervaltime: 1.0\n",
      "plotfileintervalz: 1.7976931348623157e+308\n",
      "refine_cutoff_1: 0.8\n",
      "refine_cutoff_2: 0.8\n",
      "refine_cutoff_3: 0.8\n",
      "refine_cutoff_4: 0.8\n",
      "refine_filter_1: 0.01\n",
      "refine_filter_2: 0.01\n",
      "refine_filter_3: 0.01\n",
      "refine_filter_4: 0.01\n",
      "rss_limit: -1.0\n",
      "sim_inletbuffer: 1.0\n",
      "sim_inletgrowthrate: 4.0\n",
      "sim_inletsink: -1.0\n",
      "sim_nucseedradius: 0.1\n",
      "sim_outletbuffer: 1.0\n",
      "sim_outletgrowthrate: 4.0\n",
      "sim_outletsink: -1.0\n",
      "small: 1e-10\n",
      "smalle: 1e-10\n",
      "smallp: 1e-10\n",
      "smallt: 1e-10\n",
      "smallu: 1e-10\n",
      "smallx: 1e-10\n",
      "smlrho: 1e-10\n",
      "tinitial: 0.0\n",
      "tmax: 200.0\n",
      "tstep_change_factor: 1.01\n",
      "wall_clock_checkpoint: 43200.0\n",
      "wall_clock_time_limit: 604800.0\n",
      "x_refine_center: 0.0\n",
      "xmax: 6.0\n",
      "xmin: -6.0\n",
      "y_refine_center: 0.0\n",
      "ymax: 12.0\n",
      "ymin: 0.0\n",
      "zfinal: 0.0\n",
      "zinitial: -1.0\n",
      "z_refine_center: 0.0\n",
      "zmax: 6.0\n",
      "zmin: -6.0\n",
      "\n",
      "bndpriorityone: 1\n",
      "bndprioritythree: 3\n",
      "bndprioritytwo: 2\n",
      "checkpointfileintervalstep: 0\n",
      "checkpointfilenumber: 0\n",
      "dr_abortpause: 2\n",
      "dr_dtminbelowaction: 1\n",
      "drift_break_inst: 0\n",
      "drift_trunc_mantissa: 2\n",
      "drift_verbose_inst: 0\n",
      "fileformatversion: 9\n",
      "forcedplotfilenumber: 0\n",
      "gr_amrexmg_cg_verbose: 0\n",
      "gr_amrexmg_linop_maxorder: 2\n",
      "gr_amrexmg_max_fmg_iter: 0\n",
      "gr_amrexmg_max_grid_size: 16\n",
      "gr_amrexmg_max_iter: 200\n",
      "gr_amrexmg_max_level: 1\n",
      "gr_amrexmg_n_cell: 32\n",
      "gr_amrexmg_prob_type: 1\n",
      "gr_amrexmg_ref_ratio: 2\n",
      "gr_amrexmg_verbose: 2\n",
      "gr_amrex_verbosity: 1\n",
      "gr_lrefmaxtimevalue_1: -1\n",
      "gr_lrefmaxtimevalue_10: -1\n",
      "gr_lrefmaxtimevalue_11: -1\n",
      "gr_lrefmaxtimevalue_12: -1\n",
      "gr_lrefmaxtimevalue_13: -1\n",
      "gr_lrefmaxtimevalue_14: -1\n",
      "gr_lrefmaxtimevalue_15: -1\n",
      "gr_lrefmaxtimevalue_16: -1\n",
      "gr_lrefmaxtimevalue_17: -1\n",
      "gr_lrefmaxtimevalue_18: -1\n",
      "gr_lrefmaxtimevalue_19: -1\n",
      "gr_lrefmaxtimevalue_2: -1\n",
      "gr_lrefmaxtimevalue_20: -1\n",
      "gr_lrefmaxtimevalue_3: -1\n",
      "gr_lrefmaxtimevalue_4: -1\n",
      "gr_lrefmaxtimevalue_5: -1\n",
      "gr_lrefmaxtimevalue_6: -1\n",
      "gr_lrefmaxtimevalue_7: -1\n",
      "gr_lrefmaxtimevalue_8: -1\n",
      "gr_lrefmaxtimevalue_9: -1\n",
      "gr_restrictallmethod: 3\n",
      "gr_sanitizedatamode: 4\n",
      "gr_tilesizex: 16\n",
      "gr_tilesizey: 16\n",
      "gr_tilesizez: 1\n",
      "iprocs: 1\n",
      "ins_cflflg: 1\n",
      "ins_isgs: 0\n",
      "ins_velprolongmethod: 1\n",
      "interpol_order: 2\n",
      "jprocs: 1\n",
      "kprocs: 1\n",
      "lrefine_del: 0\n",
      "lrefine_max: 1\n",
      "lrefine_min: 1\n",
      "max_particles_per_blk: 100\n",
      "memory_stat_freq: 100000\n",
      "meshcopycount: 1\n",
      "min_particles_per_blk: 1\n",
      "mph_extpit: 5\n",
      "mph_lsit: 2\n",
      "nbegin: 1\n",
      "nblockx: 24\n",
      "nblocky: 24\n",
      "nblockz: 24\n",
      "nend: 10000000\n",
      "nrefs: 10000000\n",
      "nsteptotalsts: 5\n",
      "outputsplitnum: 1\n",
      "plotfileintervalstep: 0\n",
      "plotfilenumber: 0\n",
      "refine_var_count: 4\n",
      "rolling_checkpoint: 10000\n",
      "sim_numheaters: 1\n",
      "sweeporder: 123\n",
      "wr_integrals_freq: 1\n"
     ]
    }
   ],
   "source": [
    "twall_100 = h5py.File('C:/Users/tphar/Documents/2024-II/AutoregressiveNeuralOperators/data/PoolBoiling-SubCooled-FC72-2D/Twall-100.hdf5', 'r')\n",
    "\n",
    "# Print all keys in the file\n",
    "print(\"Keys in the HDF5 file:\")\n",
    "for key in twall_100.keys():\n",
    "    print(key)\n",
    "\n",
    "# Access one specific dataset for further processing\n",
    "real_runtime_params = twall_100['real-runtime-params'][:]\n",
    "key0, val0 = real_runtime_params[0]\n",
    "int_runtime_params = twall_100['int-runtime-params'][:]\n",
    "key1, val1 = real_runtime_params[0]\n",
    "\n",
    "print(f'\\nMetadata size: {real_runtime_params.shape}')\n",
    "print(f'Key type: {type(key0)}')\n",
    "print(f'Val type: {type(val0)}')\n",
    "\n",
    "def key_to_str(key):\n",
    "    # Convert byte string to a standard python utf-8 string.\n",
    "    return key.decode('utf-8').strip()\n",
    "\n",
    "# Convert to a dict of (string, float64)\n",
    "runtime_param_dict = dict([(key_to_str(key), val) for (key, val) in real_runtime_params])\n",
    "int_param_dict = dict([(key_to_str(key), val) for (key, val) in int_runtime_params])\n",
    "\n",
    "# Print the Reynolds number\n",
    "inv_reynolds = runtime_param_dict['ins_invreynolds']\n",
    "print(f'\\nReynolds Number: {1 / inv_reynolds}')\n",
    "print()\n",
    "for key, val in runtime_param_dict.items():\n",
    "    print(f'{key}: {val}')\n",
    "\n",
    "print()\n",
    "\n",
    "for key, val in int_param_dict.items():\n",
    "    print(f'{key}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 100\n",
      "Val batches: 25\n"
     ]
    }
   ],
   "source": [
    "dataPath = 'C:/Users/tphar/Documents/2024-II/AutoregressiveNeuralOperators/data/PoolBoiling-SubCooled-FC72-2D/'\n",
    "downsampled = True\n",
    "\n",
    "dataPath = dataPath + 'downsampled_redimensionalized/' if downsampled else dataPath\n",
    "train_files = ['Twall-95.hdf5', 'Twall-98.hdf5', 'Twall-103.hdf5', 'Twall-106.hdf5']\n",
    "train_files = [dataPath + file for file in train_files]\n",
    "\n",
    "val_files = ['Twall-100.hdf5']\n",
    "val_files = [dataPath + file for file in val_files]\n",
    "\n",
    "train_dataset = ConcatDataset(SimpleLoaderBubbleML(file) for file in train_files)\n",
    "fullstack_train = SimpleLoaderBubbleML(train_files[1])\n",
    "val_dataset = ConcatDataset(SimpleLoaderBubbleML(file) for file in val_files)\n",
    "fullstack_val = SimpleLoaderBubbleML(val_files[0])\n",
    "#dataset = SimpleLoaderBubbleML(files[0])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# take first 60% of the data for training, do not shuffle and take last 20% for validation\n",
    "\n",
    "#total_size = len(dataset)\n",
    "#train_size = int(0.8 * len(dataset))\n",
    "#val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "#train_indices, val_indices = random_split(range(total_size), [train_size, val_size])\n",
    "\n",
    "\n",
    "# Create Subset datasets to keep indices for mapping back to the original data\n",
    "#train_dataset = Subset(dataset, train_indices.indices)\n",
    "#val_dataset = Subset(dataset, val_indices.indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Train batches: {len(train_dataloader)}')\n",
    "print(f'Val batches: {len(val_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,703,107 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "newmodel = True\n",
    "if newmodel:\n",
    "    model = UNet2D(3, 3, features=(64, 128, 256)).to(DEVICE)\n",
    "else:\n",
    "    model = UNet2D(3, 3, features=(64, 128, 256)).to(DEVICE)\n",
    "    model = model.load_state_dict(torch.load('../models/unet_model_BubbleML.pth'))\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = UNet2D(3, 3, features=(64, 128, 256)).to(DEVICE)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.4756, Validation Loss: 2.5330\n",
      "Epoch 2/100 - Train Loss: 0.4338, Validation Loss: 2.4602\n",
      "Epoch 3/100 - Train Loss: 0.4107, Validation Loss: 2.4902\n",
      "Epoch 4/100 - Train Loss: 0.3770, Validation Loss: 2.5049\n",
      "Epoch 5/100 - Train Loss: 0.3478, Validation Loss: 2.4640\n",
      "Epoch 6/100 - Train Loss: 0.3328, Validation Loss: 2.4485\n",
      "Epoch 7/100 - Train Loss: 0.3084, Validation Loss: 2.4444\n",
      "Epoch 8/100 - Train Loss: 0.2989, Validation Loss: 2.4632\n",
      "Epoch 9/100 - Train Loss: 0.2877, Validation Loss: 2.5271\n",
      "Epoch 10/100 - Train Loss: 0.2795, Validation Loss: 2.5075\n",
      "Epoch 11/100 - Train Loss: 0.2754, Validation Loss: 2.5080\n",
      "Epoch 12/100 - Train Loss: 0.2708, Validation Loss: 2.5167\n",
      "Epoch 13/100 - Train Loss: 0.2587, Validation Loss: 2.4648\n",
      "Epoch 14/100 - Train Loss: 0.2501, Validation Loss: 2.4834\n",
      "Epoch 15/100 - Train Loss: 0.2373, Validation Loss: 2.4908\n",
      "Epoch 16/100 - Train Loss: 0.2260, Validation Loss: 2.4702\n",
      "Epoch 17/100 - Train Loss: 0.2312, Validation Loss: 2.4762\n",
      "Epoch 18/100 - Train Loss: 0.2116, Validation Loss: 2.5142\n",
      "Epoch 19/100 - Train Loss: 0.2192, Validation Loss: 2.4538\n",
      "Epoch 20/100 - Train Loss: 0.2108, Validation Loss: 2.4668\n",
      "Epoch 21/100 - Train Loss: 0.2012, Validation Loss: 2.4485\n",
      "Epoch 22/100 - Train Loss: 0.1963, Validation Loss: 2.4831\n",
      "Epoch 23/100 - Train Loss: 0.1804, Validation Loss: 2.4670\n",
      "Epoch 24/100 - Train Loss: 0.1930, Validation Loss: 2.4689\n",
      "Epoch 25/100 - Train Loss: 0.1900, Validation Loss: 2.4757\n",
      "Epoch 26/100 - Train Loss: 0.1844, Validation Loss: 2.4897\n",
      "Epoch 27/100 - Train Loss: 0.1809, Validation Loss: 2.4643\n",
      "Epoch 28/100 - Train Loss: 0.1760, Validation Loss: 2.4560\n",
      "Epoch 29/100 - Train Loss: 0.1669, Validation Loss: 2.4862\n",
      "Epoch 30/100 - Train Loss: 0.1714, Validation Loss: 2.4857\n",
      "Epoch 31/100 - Train Loss: 0.1691, Validation Loss: 2.4371\n",
      "Epoch 32/100 - Train Loss: 0.1695, Validation Loss: 2.4708\n",
      "Epoch 33/100 - Train Loss: 0.1646, Validation Loss: 2.4724\n",
      "Epoch 34/100 - Train Loss: 0.1626, Validation Loss: 2.5470\n",
      "Epoch 35/100 - Train Loss: 0.1574, Validation Loss: 2.4318\n",
      "Epoch 36/100 - Train Loss: 0.1593, Validation Loss: 2.4839\n",
      "Epoch 37/100 - Train Loss: 0.1519, Validation Loss: 2.4764\n",
      "Epoch 38/100 - Train Loss: 0.1471, Validation Loss: 2.5148\n",
      "Epoch 39/100 - Train Loss: 0.1401, Validation Loss: 2.4930\n",
      "Epoch 40/100 - Train Loss: 0.1347, Validation Loss: 2.4583\n",
      "Epoch 41/100 - Train Loss: 0.1389, Validation Loss: 2.4826\n",
      "Epoch 42/100 - Train Loss: 0.1414, Validation Loss: 2.4865\n",
      "Epoch 43/100 - Train Loss: 0.1408, Validation Loss: 2.4796\n",
      "Epoch 44/100 - Train Loss: 0.1356, Validation Loss: 2.4943\n",
      "Epoch 45/100 - Train Loss: 0.1322, Validation Loss: 2.4488\n",
      "Epoch 46/100 - Train Loss: 0.1280, Validation Loss: 2.4389\n",
      "Epoch 47/100 - Train Loss: 0.1312, Validation Loss: 2.4752\n",
      "Epoch 48/100 - Train Loss: 0.1367, Validation Loss: 2.4733\n",
      "Epoch 49/100 - Train Loss: 0.1307, Validation Loss: 2.5018\n",
      "Epoch 50/100 - Train Loss: 0.1425, Validation Loss: 2.5087\n",
      "Epoch 51/100 - Train Loss: 0.1387, Validation Loss: 2.4630\n",
      "Epoch 52/100 - Train Loss: 0.1266, Validation Loss: 2.4688\n",
      "Epoch 53/100 - Train Loss: 0.1139, Validation Loss: 2.4724\n",
      "Epoch 54/100 - Train Loss: 0.1120, Validation Loss: 2.5262\n",
      "Epoch 55/100 - Train Loss: 0.1092, Validation Loss: 2.4305\n",
      "Epoch 56/100 - Train Loss: 0.1086, Validation Loss: 2.4653\n",
      "Epoch 57/100 - Train Loss: 0.1045, Validation Loss: 2.4651\n",
      "Epoch 58/100 - Train Loss: 0.1050, Validation Loss: 2.4907\n",
      "Epoch 59/100 - Train Loss: 0.1111, Validation Loss: 2.4731\n",
      "Epoch 60/100 - Train Loss: 0.1108, Validation Loss: 2.4225\n",
      "Epoch 61/100 - Train Loss: 0.1069, Validation Loss: 2.4705\n",
      "Epoch 62/100 - Train Loss: 0.1058, Validation Loss: 2.4475\n",
      "Epoch 63/100 - Train Loss: 0.1043, Validation Loss: 2.4576\n",
      "Epoch 64/100 - Train Loss: 0.1035, Validation Loss: 2.4277\n",
      "Epoch 65/100 - Train Loss: 0.0967, Validation Loss: 2.4329\n",
      "Epoch 66/100 - Train Loss: 0.0968, Validation Loss: 2.4514\n",
      "Epoch 67/100 - Train Loss: 0.0952, Validation Loss: 2.4391\n",
      "Epoch 68/100 - Train Loss: 0.0990, Validation Loss: 2.4784\n",
      "Epoch 69/100 - Train Loss: 0.1049, Validation Loss: 2.4681\n",
      "Epoch 70/100 - Train Loss: 0.1065, Validation Loss: 2.4744\n",
      "Epoch 71/100 - Train Loss: 0.0952, Validation Loss: 2.4848\n",
      "Epoch 72/100 - Train Loss: 0.0964, Validation Loss: 2.4521\n",
      "Epoch 73/100 - Train Loss: 0.0914, Validation Loss: 2.4515\n",
      "Epoch 74/100 - Train Loss: 0.0904, Validation Loss: 2.4567\n",
      "Epoch 75/100 - Train Loss: 0.0864, Validation Loss: 2.4827\n",
      "Epoch 76/100 - Train Loss: 0.0869, Validation Loss: 2.4491\n",
      "Epoch 77/100 - Train Loss: 0.0894, Validation Loss: 2.4671\n",
      "Epoch 78/100 - Train Loss: 0.0895, Validation Loss: 2.4528\n",
      "Epoch 79/100 - Train Loss: 0.0825, Validation Loss: 2.4875\n",
      "Epoch 80/100 - Train Loss: 0.0844, Validation Loss: 2.4353\n",
      "Epoch 81/100 - Train Loss: 0.0816, Validation Loss: 2.4527\n",
      "Epoch 82/100 - Train Loss: 0.0861, Validation Loss: 2.4642\n",
      "Epoch 83/100 - Train Loss: 0.0841, Validation Loss: 2.4421\n",
      "Epoch 84/100 - Train Loss: 0.0829, Validation Loss: 2.4737\n",
      "Epoch 85/100 - Train Loss: 0.0916, Validation Loss: 2.4626\n",
      "Epoch 86/100 - Train Loss: 0.0902, Validation Loss: 2.5020\n",
      "Epoch 87/100 - Train Loss: 0.0882, Validation Loss: 2.4651\n",
      "Epoch 88/100 - Train Loss: 0.0832, Validation Loss: 2.4776\n",
      "Epoch 89/100 - Train Loss: 0.0841, Validation Loss: 2.4815\n",
      "Epoch 90/100 - Train Loss: 0.0790, Validation Loss: 2.4701\n",
      "Epoch 91/100 - Train Loss: 0.0750, Validation Loss: 2.4946\n",
      "Epoch 92/100 - Train Loss: 0.0756, Validation Loss: 2.4846\n",
      "Epoch 93/100 - Train Loss: 0.0800, Validation Loss: 2.4481\n",
      "Epoch 94/100 - Train Loss: 0.0800, Validation Loss: 2.4663\n",
      "Epoch 95/100 - Train Loss: 0.0757, Validation Loss: 2.5035\n",
      "Epoch 96/100 - Train Loss: 0.0754, Validation Loss: 2.5169\n",
      "Epoch 97/100 - Train Loss: 0.0706, Validation Loss: 2.4506\n",
      "Epoch 98/100 - Train Loss: 0.0681, Validation Loss: 2.4928\n",
      "Epoch 99/100 - Train Loss: 0.0732, Validation Loss: 2.4953\n",
      "Epoch 100/100 - Train Loss: 0.0743, Validation Loss: 2.5042\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "wandb_on = False\n",
    "\n",
    "if wandb_on:\n",
    "    wandb.init(project=\"BubbleML_Unet\", config={\n",
    "        \"epochs\": EPOCHS\n",
    "    })\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss = one_epoch_train_simple(model, train_dataloader, optimizer, DEVICE)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    avg_val_loss = one_epoch_val_simple(model, val_dataloader, DEVICE)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    if wandb_on:\n",
    "        wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "            })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "if wandb_on:\n",
    "    wandb.finish()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_model(model, data, device, start_index, timesteps):\n",
    "    model.eval()\n",
    "\n",
    "    input = data[start_index].to(device).float().unsqueeze(0)\n",
    "    predictions = [input.cpu()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(timesteps - 1):\n",
    "            pred = model(input)\n",
    "            predictions.append(pred.cpu())\n",
    "            input = pred\n",
    "\n",
    "    stacked_predictions = torch.stack(predictions, dim=0).squeeze(1)\n",
    "\n",
    "    return stacked_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(true_data, predictions, start_index, timesteps, output_path, vertical=False):\n",
    "    if vertical:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(5, 10))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    def update_frame(i):\n",
    "        ax[0].clear()\n",
    "        ax[1].clear()\n",
    "\n",
    "        imgtrue = np.flipud(true_data[i + start_index, 0, :, :].numpy())\n",
    "        imgpred = np.flipud(predictions[i, 0, :, :].numpy())\n",
    "\n",
    "        ax[0].imshow(imgtrue, cmap='jet', vmin=0, vmax=1)\n",
    "        ax[0].set_title(\"True\")\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(imgpred, cmap='jet', vmin=0, vmax=1)\n",
    "        ax[1].set_title(\"Prediction\")\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        fig.suptitle(f\"Step {i + 1}/{timesteps}\")\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_frame, frames=timesteps, interval=500)\n",
    "    ani.save(output_path, writer='ffmpeg', fps=10)\n",
    "    plt.close()\n",
    "heatertemp = 100\n",
    "rollout_val = rollout_model(model, fullstack_val.get_full_stack(), DEVICE, 20, 40)\n",
    "rollout_val = rollout_val / heatertemp\n",
    "fullstack_val_norm = fullstack_val.get_full_stack() / heatertemp\n",
    "create_gif(fullstack_val_norm, rollout_val, 20, 40, '../output/valUNet_hor.gif', vertical=False)\n",
    "fullstack_train_norm = fullstack_train.get_full_stack() / heatertemp\n",
    "rollout_train = rollout_model(model, fullstack_train.get_full_stack(), DEVICE, 20, 40)\n",
    "rollout_train = rollout_train / heatertemp\n",
    "create_gif(fullstack_train_norm, rollout_train, 20, 40, '../output/trainUNet_hor.gif', vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#torch.save(model.state_dict(), '../models/unet_model_BubbleML.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full stack shape: torch.Size([201, 4, 384, 384])\n",
      "Number of timesteps: 201\n",
      "Spatial dimensions: (384, 384)\n"
     ]
    }
   ],
   "source": [
    "class FullLoaderBubbleML:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = h5py.File(self.filename, 'r')\n",
    "        self.timesteps = self.data[TEMPERATURE][:].shape[0]\n",
    "        self.spatial_dims = self.data[TEMPERATURE][:].shape[1:]  # Get spatial dimensions (Y, X)\n",
    "    \n",
    "    def get_full_stack(self):\n",
    "        \"\"\"\n",
    "        Retrieve the full temporal domain stack for the entire file.\n",
    "        Returns a tensor with shape: [timesteps, channels, Y, X].\n",
    "        \"\"\"\n",
    "        # Load temperature, velx, vely, and pressure data from the HDF5 file\n",
    "        temp_data = torch.from_numpy(self.data[TEMPERATURE][:])  # Shape: [timesteps, Y, X]\n",
    "        velx_data = torch.from_numpy(self.data[VELX][:])         # Shape: [timesteps, Y, X]\n",
    "        vely_data = torch.from_numpy(self.data[VELY][:])         # Shape: [timesteps, Y, X]\n",
    "        pres_data = torch.from_numpy(self.data[PRESSURE][:])     # Shape: [timesteps, Y, X]\n",
    "        \n",
    "        # Stack the data along the channel dimension\n",
    "        full_stack = torch.stack((temp_data, velx_data, vely_data, pres_data), dim=1)  # Shape: [timesteps, channels, Y, X]\n",
    "        return full_stack\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieve metadata information such as the number of timesteps and spatial dimensions.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"spatial_dims\": self.spatial_dims\n",
    "        }\n",
    "    \n",
    "# Initialize the loader\n",
    "filename = files[0]\n",
    "loader = FullLoaderBubbleML(filename)\n",
    "\n",
    "# Get the full stack\n",
    "full_stack = loader.get_full_stack()\n",
    "print(f\"Full stack shape: {full_stack.shape}\")  # Expected: [timesteps, channels, Y, X]\n",
    "\n",
    "# Access metadata\n",
    "metadata = loader.get_metadata()\n",
    "print(f\"Number of timesteps: {metadata['timesteps']}\")\n",
    "print(f\"Spatial dimensions: {metadata['spatial_dims']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class AutoregressiveModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "def train_autoregressive(model, dataloader, optimizer, loss_fn, max_rollout_steps):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch  # inputs: [batch_size, sequence_length, input_dim], targets: [batch_size, sequence_length, target_dim]\n",
    "        batch_size, sequence_length, _ = inputs.size()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = []\n",
    "        current_input = inputs[:, 0:1]  # Start with the first timestep\n",
    "        for t in range(max_rollout_steps):\n",
    "            output = model(current_input)  # Predict the next timestep\n",
    "            predictions.append(output)\n",
    "            current_input = output  # Feed the prediction as the next input\n",
    "\n",
    "        # Stack all predictions along the time dimension\n",
    "        predictions = torch.cat(predictions, dim=1)  # [batch_size, max_rollout_steps, target_dim]\n",
    "\n",
    "        # Compute loss only on the final prediction\n",
    "        loss = loss_fn(predictions[:, -1], targets[:, max_rollout_steps - 1])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy dataset\n",
    "    class DummyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, num_samples, sequence_length, input_dim):\n",
    "            self.data = torch.randn(num_samples, sequence_length, input_dim)\n",
    "            self.targets = torch.randn(num_samples, sequence_length, input_dim)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx], self.targets[idx]\n",
    "\n",
    "    dataset = DummyDataset(num_samples=100, sequence_length=10, input_dim=3)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Model, optimizer, loss function\n",
    "    model = AutoregressiveModel(input_size=3, hidden_size=16, output_size=3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Train\n",
    "    train_autoregressive(model, dataloader, optimizer, loss_fn, max_rollout_steps=5)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
