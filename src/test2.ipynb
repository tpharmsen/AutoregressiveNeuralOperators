{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12686b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following is a gpu mig bug fix\n",
    "import os\n",
    "import subprocess\n",
    "if \"MIG\" in subprocess.check_output([\"nvidia-smi\", \"-L\"], text=True):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aae03a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 2, 128, 128])\n",
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "patch_size = (8,8)\n",
    "H = W = 128\n",
    "unpatchify = nn.Fold(output_size=(H, W), kernel_size=patch_size, stride=patch_size)\n",
    "x = torch.randn(192,128,256)\n",
    "y = unpatchify(x)\n",
    "print(y.shape)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ca88644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from dataloaders.utils import spatial_resample\n",
    "import numpy as np\n",
    "\n",
    "class AmiraReaderFromAM:\n",
    "    def __init__(self, filepaths, resample_shape=128, resample_mode='fourier', timesample=5):\n",
    "        self.data_list = []\n",
    "        self.traj_list = []\n",
    "        self.ts = None\n",
    "        self.resample_shape = resample_shape\n",
    "        self.resample_mode = resample_mode\n",
    "        self.name = None\n",
    "        self.vel_scale = None\n",
    "        self.dt = timesample\n",
    "        \n",
    "        for filepath in filepaths:\n",
    "            #print(filepath)\n",
    "            data = torch.from_numpy(self.read_amira_binary_mesh(filepath).copy())\n",
    "            data = data.permute(0,3,1,2)\n",
    "            data = spatial_resample(data, self.resample_shape, self.resample_mode)\n",
    "            self.data_list.append(data)\n",
    "            self.traj_list.append(torch.tensor(1))\n",
    "            if self.ts is None:\n",
    "                self.ts = data.shape[0]\n",
    "        \n",
    "        self.data = torch.stack(self.data_list, dim=0)\n",
    "        #print(self.data.shape)\n",
    "        self.traj = sum(self.traj_list)\n",
    "\n",
    "    def read_amira_binary_mesh(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        # first occurrence of \"@1\"\n",
    "        first_marker_idx = raw_data.find(b'@1')\n",
    "        if first_marker_idx == -1:\n",
    "            raise ValueError(\"Could not find binary data section in Amira file.\")\n",
    "        # second occurrence of \"@1\"\n",
    "        second_marker_idx = raw_data.find(b'@1', first_marker_idx + 2)\n",
    "        if second_marker_idx == -1:\n",
    "            raise ValueError(\"Could not find second binary data section in Amira file.\")\n",
    "        data_start = second_marker_idx + 4  # Skip '@1\\n'\n",
    "        binary_data = raw_data[data_start:]\n",
    "        lattice_shape = (1001, 512, 512, 2)\n",
    "        float_data = np.frombuffer(binary_data, dtype=np.float32)\n",
    "        float_data = float_data.reshape(lattice_shape)\n",
    "        return float_data\n",
    "        \n",
    "    def get_single_traj(self, idx):\n",
    "        full = self.data[idx][::self.dt]\n",
    "        return full\n",
    "    \n",
    "    def normalize_velocity(self, vel_scale):\n",
    "        self.data = self.data / vel_scale\n",
    "        self.vel_scale = vel_scale\n",
    "\n",
    "    def absmax_vel(self):\n",
    "        return self.data.abs().max()\n",
    "\n",
    "class AmiraDatasetFromAM(Dataset):\n",
    "    def __init__(self, reader: AmiraReaderFromAM, temporal_bundling = 1, forward_steps = 1):\n",
    "        self.reader = reader\n",
    "        self.traj = reader.traj\n",
    "        self.dt = reader.dt\n",
    "        self.ts = reader.ts\n",
    "        self.tb = temporal_bundling\n",
    "        self.fs = forward_steps\n",
    "        self.lenpertraj = self.reader.ts - (1 + self.fs) * self.reader.dt * self.tb + self.reader.dt\n",
    "        self.idx_window = self.reader.dt * self.tb\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.reader.traj * self.lenpertraj\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx = idx // self.lenpertraj\n",
    "        ts_idx = idx % self.lenpertraj\n",
    "        print(idx, traj_idx, ts_idx, self.lenpertraj, self.reader.traj, self.reader.ts, self.reader.dt, self.tb, self.fs)\n",
    "        front = self.reader.data[traj_idx][ts_idx : ts_idx + self.idx_window : self.reader.dt]\n",
    "        label = self.reader.data[traj_idx][ts_idx + self.fs * self.idx_window : ts_idx + (self.fs + 1) * self.idx_window : self.reader.dt]\n",
    "        return front, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "48885944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5a7dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = AmiraReaderFromAM(['../datasets/prjs1359/AmiraSet/0000.am', '../datasets/prjs1359/AmiraSet/4400.am'], resample_shape=128, resample_mode='fourier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26c2de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "Dataset length: 976\n",
      "[976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951]\n",
      "[931, 935, 939, 943, 947, 951, 955, 959, 963, 967, 971, 975, 979, 983, 987, 991, 995, 999, 1003, 1007, 1011, 1015, 1019, 1023, 1027, 1031, 1035, 1039, 1043, 1047, 1051, 1055, 1059, 1063, 1067, 1071, 1075, 1079, 1083, 1087, 1091, 1095, 1099, 1103, 1107, 1111, 1115, 1119, 1123, 1127, 1131, 1135, 1139, 1143, 1147, 1151, 1155, 1159, 1163, 1167, 1171, 1175, 1179, 1183, 1187, 1191, 1195, 1199, 1203, 1207, 1211, 1215, 1219, 1223, 1227, 1231, 1235, 1239, 1243, 1247, 1251, 1255, 1259, 1263, 1267, 1271, 1275, 1279, 1283, 1287, 1291, 1295, 1299, 1303, 1307, 1311, 1315, 1319, 1323, 1327, 1331, 1335, 1339, 1343, 1347, 1351, 1355, 1359, 1363, 1367, 1371, 1375, 1379, 1383, 1387, 1391, 1395, 1399, 1403, 1407, 1411, 1415, 1419, 1423, 1427, 1431, 1435, 1439, 1443, 1447, 1451, 1455, 1459, 1463, 1467, 1471, 1475, 1479, 1483, 1487, 1491, 1495, 1499, 1503, 1507, 1511, 1515, 1519, 1523, 1527, 1531, 1535, 1539, 1543, 1547, 1551, 1555, 1559, 1563, 1567, 1571, 1575, 1579, 1583, 1587, 1591, 1595, 1599, 1603, 1607, 1611, 1615, 1619, 1623, 1627, 1631, 1635, 1639, 1643, 1647, 1651, 1655, 1659, 1663, 1667, 1671, 1675, 1679, 1683, 1687, 1691, 1695, 1699, 1703, 1707, 1711, 1715, 1719, 1723, 1727, 1731, 1735, 1739, 1743, 1747, 1751, 1755, 1759, 1763, 1767, 1771, 1775, 1779, 1783, 1787, 1791, 1795, 1799, 1803, 1807, 1811, 1815, 1819, 1823, 1827, 1831, 1835, 1839, 1843, 1847, 1851, 1855, 1859]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tb = 3\n",
    "plotting = False\n",
    "dataset = AmiraDatasetFromAM(reader, temporal_bundling=tb, forward_steps=1)\n",
    "dataset2 = AmiraDatasetFromAM(reader, temporal_bundling=tb, forward_steps=4)\n",
    "\n",
    "class ZeroShotSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, train_ratio=0.8, split=\"train\", seed=227, forward_steps=1):\n",
    "        torch.manual_seed(seed) \n",
    "        num_train = int(dataset.traj * train_ratio)\n",
    "        shuffled_trajs = torch.randperm(dataset.traj).tolist() \n",
    "        train_trajs = shuffled_trajs[:num_train]\n",
    "        self.val_trajs = shuffled_trajs[num_train:]\n",
    "        train_indices = [t * dataset.lenpertraj + ts for t in train_trajs for ts in range(0, dataset.lenpertraj, forward_steps)]\n",
    "        val_indices = [t * dataset.lenpertraj + ts for t in self.val_trajs for ts in range(0, dataset.lenpertraj, forward_steps)]\n",
    "        self.indices = train_indices if split == \"train\" else val_indices\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def random_val_traj(self):\n",
    "        return self.val_trajs[torch.randint(0, len(self.val_trajs), (1,)).item()]\n",
    "sampler0 = ZeroShotSampler(dataset, train_ratio=0.8, split=\"val\", seed=227, forward_steps=1)\n",
    "sampler1 = ZeroShotSampler(dataset2, train_ratio=0.8, split=\"val\", seed=227, forward_steps=4)\n",
    "loader0 = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset, sampler0.indices), batch_size=12, shuffle=False)\n",
    "loader1 = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset2, sampler1.indices), batch_size=12, shuffle=False)\n",
    "print(sampler0.val_trajs)\n",
    "print(\"Dataset length:\", dataset.lenpertraj)\n",
    "'''\n",
    "for idx, (front, label) in enumerate(loader1):\n",
    "    print(f\"Batch {idx}:\", end=\" \")\n",
    "    print(\"Front shape:\", front.shape, end=\" \")\n",
    "    print(\"Label shape:\", label.shape)\n",
    "    if plotting:\n",
    "        fig, ax = plt.subplots(2, tb, figsize=(16, 4))\n",
    "        for i in range(tb):\n",
    "            ax[0,i].imshow(front[0, i, 1, :, :].cpu().numpy())\n",
    "            ax[1,i].imshow(label[0, i, 1, :, :].cpu().numpy())\n",
    "            ax[0,i].set_title(f\"Front {i}\")\n",
    "            ax[0,i].axis('off')\n",
    "            ax[1,i].set_title(f\"Label {i}\")\n",
    "            ax[1,i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #if idx == 15:\n",
    "    #    break\n",
    "'''\n",
    "print(sampler0.indices)\n",
    "print(sampler1.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d0acad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931 1 0 931 tensor(2) 1001 5 3 4\n",
      "935 1 4 931 tensor(2) 1001 5 3 4\n",
      "939 1 8 931 tensor(2) 1001 5 3 4\n",
      "943 1 12 931 tensor(2) 1001 5 3 4\n",
      "947 1 16 931 tensor(2) 1001 5 3 4\n",
      "951 1 20 931 tensor(2) 1001 5 3 4\n",
      "955 1 24 931 tensor(2) 1001 5 3 4\n",
      "959 1 28 931 tensor(2) 1001 5 3 4\n",
      "963 1 32 931 tensor(2) 1001 5 3 4\n",
      "967 1 36 931 tensor(2) 1001 5 3 4\n",
      "971 1 40 931 tensor(2) 1001 5 3 4\n",
      "975 1 44 931 tensor(2) 1001 5 3 4\n",
      "979 1 48 931 tensor(2) 1001 5 3 4\n",
      "983 1 52 931 tensor(2) 1001 5 3 4\n",
      "987 1 56 931 tensor(2) 1001 5 3 4\n",
      "991 1 60 931 tensor(2) 1001 5 3 4\n",
      "995 1 64 931 tensor(2) 1001 5 3 4\n",
      "999 1 68 931 tensor(2) 1001 5 3 4\n",
      "1003 1 72 931 tensor(2) 1001 5 3 4\n",
      "1007 1 76 931 tensor(2) 1001 5 3 4\n",
      "1011 1 80 931 tensor(2) 1001 5 3 4\n",
      "1015 1 84 931 tensor(2) 1001 5 3 4\n",
      "1019 1 88 931 tensor(2) 1001 5 3 4\n",
      "1023 1 92 931 tensor(2) 1001 5 3 4\n",
      "1027 1 96 931 tensor(2) 1001 5 3 4\n",
      "1031 1 100 931 tensor(2) 1001 5 3 4\n",
      "1035 1 104 931 tensor(2) 1001 5 3 4\n",
      "1039 1 108 931 tensor(2) 1001 5 3 4\n",
      "1043 1 112 931 tensor(2) 1001 5 3 4\n",
      "1047 1 116 931 tensor(2) 1001 5 3 4\n",
      "1051 1 120 931 tensor(2) 1001 5 3 4\n",
      "1055 1 124 931 tensor(2) 1001 5 3 4\n",
      "1059 1 128 931 tensor(2) 1001 5 3 4\n",
      "1063 1 132 931 tensor(2) 1001 5 3 4\n",
      "1067 1 136 931 tensor(2) 1001 5 3 4\n",
      "1071 1 140 931 tensor(2) 1001 5 3 4\n",
      "1075 1 144 931 tensor(2) 1001 5 3 4\n",
      "1079 1 148 931 tensor(2) 1001 5 3 4\n",
      "1083 1 152 931 tensor(2) 1001 5 3 4\n",
      "1087 1 156 931 tensor(2) 1001 5 3 4\n",
      "1091 1 160 931 tensor(2) 1001 5 3 4\n",
      "1095 1 164 931 tensor(2) 1001 5 3 4\n",
      "1099 1 168 931 tensor(2) 1001 5 3 4\n",
      "1103 1 172 931 tensor(2) 1001 5 3 4\n",
      "1107 1 176 931 tensor(2) 1001 5 3 4\n",
      "1111 1 180 931 tensor(2) 1001 5 3 4\n",
      "1115 1 184 931 tensor(2) 1001 5 3 4\n",
      "1119 1 188 931 tensor(2) 1001 5 3 4\n",
      "1123 1 192 931 tensor(2) 1001 5 3 4\n",
      "1127 1 196 931 tensor(2) 1001 5 3 4\n",
      "1131 1 200 931 tensor(2) 1001 5 3 4\n",
      "1135 1 204 931 tensor(2) 1001 5 3 4\n",
      "1139 1 208 931 tensor(2) 1001 5 3 4\n",
      "1143 1 212 931 tensor(2) 1001 5 3 4\n",
      "1147 1 216 931 tensor(2) 1001 5 3 4\n",
      "1151 1 220 931 tensor(2) 1001 5 3 4\n",
      "1155 1 224 931 tensor(2) 1001 5 3 4\n",
      "1159 1 228 931 tensor(2) 1001 5 3 4\n",
      "1163 1 232 931 tensor(2) 1001 5 3 4\n",
      "1167 1 236 931 tensor(2) 1001 5 3 4\n",
      "1171 1 240 931 tensor(2) 1001 5 3 4\n",
      "1175 1 244 931 tensor(2) 1001 5 3 4\n",
      "1179 1 248 931 tensor(2) 1001 5 3 4\n",
      "1183 1 252 931 tensor(2) 1001 5 3 4\n",
      "1187 1 256 931 tensor(2) 1001 5 3 4\n",
      "1191 1 260 931 tensor(2) 1001 5 3 4\n",
      "1195 1 264 931 tensor(2) 1001 5 3 4\n",
      "1199 1 268 931 tensor(2) 1001 5 3 4\n",
      "1203 1 272 931 tensor(2) 1001 5 3 4\n",
      "1207 1 276 931 tensor(2) 1001 5 3 4\n",
      "1211 1 280 931 tensor(2) 1001 5 3 4\n",
      "1215 1 284 931 tensor(2) 1001 5 3 4\n",
      "1219 1 288 931 tensor(2) 1001 5 3 4\n",
      "1223 1 292 931 tensor(2) 1001 5 3 4\n",
      "1227 1 296 931 tensor(2) 1001 5 3 4\n",
      "1231 1 300 931 tensor(2) 1001 5 3 4\n",
      "1235 1 304 931 tensor(2) 1001 5 3 4\n",
      "1239 1 308 931 tensor(2) 1001 5 3 4\n",
      "1243 1 312 931 tensor(2) 1001 5 3 4\n",
      "1247 1 316 931 tensor(2) 1001 5 3 4\n",
      "1251 1 320 931 tensor(2) 1001 5 3 4\n",
      "1255 1 324 931 tensor(2) 1001 5 3 4\n",
      "1259 1 328 931 tensor(2) 1001 5 3 4\n",
      "1263 1 332 931 tensor(2) 1001 5 3 4\n",
      "1267 1 336 931 tensor(2) 1001 5 3 4\n",
      "1271 1 340 931 tensor(2) 1001 5 3 4\n",
      "1275 1 344 931 tensor(2) 1001 5 3 4\n",
      "1279 1 348 931 tensor(2) 1001 5 3 4\n",
      "1283 1 352 931 tensor(2) 1001 5 3 4\n",
      "1287 1 356 931 tensor(2) 1001 5 3 4\n",
      "1291 1 360 931 tensor(2) 1001 5 3 4\n",
      "1295 1 364 931 tensor(2) 1001 5 3 4\n",
      "1299 1 368 931 tensor(2) 1001 5 3 4\n",
      "1303 1 372 931 tensor(2) 1001 5 3 4\n",
      "1307 1 376 931 tensor(2) 1001 5 3 4\n",
      "1311 1 380 931 tensor(2) 1001 5 3 4\n",
      "1315 1 384 931 tensor(2) 1001 5 3 4\n",
      "1319 1 388 931 tensor(2) 1001 5 3 4\n",
      "1323 1 392 931 tensor(2) 1001 5 3 4\n",
      "1327 1 396 931 tensor(2) 1001 5 3 4\n",
      "1331 1 400 931 tensor(2) 1001 5 3 4\n",
      "1335 1 404 931 tensor(2) 1001 5 3 4\n",
      "1339 1 408 931 tensor(2) 1001 5 3 4\n",
      "1343 1 412 931 tensor(2) 1001 5 3 4\n",
      "1347 1 416 931 tensor(2) 1001 5 3 4\n",
      "1351 1 420 931 tensor(2) 1001 5 3 4\n",
      "1355 1 424 931 tensor(2) 1001 5 3 4\n",
      "1359 1 428 931 tensor(2) 1001 5 3 4\n",
      "1363 1 432 931 tensor(2) 1001 5 3 4\n",
      "1367 1 436 931 tensor(2) 1001 5 3 4\n",
      "1371 1 440 931 tensor(2) 1001 5 3 4\n",
      "1375 1 444 931 tensor(2) 1001 5 3 4\n",
      "1379 1 448 931 tensor(2) 1001 5 3 4\n",
      "1383 1 452 931 tensor(2) 1001 5 3 4\n",
      "1387 1 456 931 tensor(2) 1001 5 3 4\n",
      "1391 1 460 931 tensor(2) 1001 5 3 4\n",
      "1395 1 464 931 tensor(2) 1001 5 3 4\n",
      "1399 1 468 931 tensor(2) 1001 5 3 4\n",
      "1403 1 472 931 tensor(2) 1001 5 3 4\n",
      "1407 1 476 931 tensor(2) 1001 5 3 4\n",
      "1411 1 480 931 tensor(2) 1001 5 3 4\n",
      "1415 1 484 931 tensor(2) 1001 5 3 4\n",
      "1419 1 488 931 tensor(2) 1001 5 3 4\n",
      "1423 1 492 931 tensor(2) 1001 5 3 4\n",
      "1427 1 496 931 tensor(2) 1001 5 3 4\n",
      "1431 1 500 931 tensor(2) 1001 5 3 4\n",
      "1435 1 504 931 tensor(2) 1001 5 3 4\n",
      "1439 1 508 931 tensor(2) 1001 5 3 4\n",
      "1443 1 512 931 tensor(2) 1001 5 3 4\n",
      "1447 1 516 931 tensor(2) 1001 5 3 4\n",
      "1451 1 520 931 tensor(2) 1001 5 3 4\n",
      "1455 1 524 931 tensor(2) 1001 5 3 4\n",
      "1459 1 528 931 tensor(2) 1001 5 3 4\n",
      "1463 1 532 931 tensor(2) 1001 5 3 4\n",
      "1467 1 536 931 tensor(2) 1001 5 3 4\n",
      "1471 1 540 931 tensor(2) 1001 5 3 4\n",
      "1475 1 544 931 tensor(2) 1001 5 3 4\n",
      "1479 1 548 931 tensor(2) 1001 5 3 4\n",
      "1483 1 552 931 tensor(2) 1001 5 3 4\n",
      "1487 1 556 931 tensor(2) 1001 5 3 4\n",
      "1491 1 560 931 tensor(2) 1001 5 3 4\n",
      "1495 1 564 931 tensor(2) 1001 5 3 4\n",
      "1499 1 568 931 tensor(2) 1001 5 3 4\n",
      "1503 1 572 931 tensor(2) 1001 5 3 4\n",
      "1507 1 576 931 tensor(2) 1001 5 3 4\n",
      "1511 1 580 931 tensor(2) 1001 5 3 4\n",
      "1515 1 584 931 tensor(2) 1001 5 3 4\n",
      "1519 1 588 931 tensor(2) 1001 5 3 4\n",
      "1523 1 592 931 tensor(2) 1001 5 3 4\n",
      "1527 1 596 931 tensor(2) 1001 5 3 4\n",
      "1531 1 600 931 tensor(2) 1001 5 3 4\n",
      "1535 1 604 931 tensor(2) 1001 5 3 4\n",
      "1539 1 608 931 tensor(2) 1001 5 3 4\n",
      "1543 1 612 931 tensor(2) 1001 5 3 4\n",
      "1547 1 616 931 tensor(2) 1001 5 3 4\n",
      "1551 1 620 931 tensor(2) 1001 5 3 4\n",
      "1555 1 624 931 tensor(2) 1001 5 3 4\n",
      "1559 1 628 931 tensor(2) 1001 5 3 4\n",
      "1563 1 632 931 tensor(2) 1001 5 3 4\n",
      "1567 1 636 931 tensor(2) 1001 5 3 4\n",
      "1571 1 640 931 tensor(2) 1001 5 3 4\n",
      "1575 1 644 931 tensor(2) 1001 5 3 4\n",
      "1579 1 648 931 tensor(2) 1001 5 3 4\n",
      "1583 1 652 931 tensor(2) 1001 5 3 4\n",
      "1587 1 656 931 tensor(2) 1001 5 3 4\n",
      "1591 1 660 931 tensor(2) 1001 5 3 4\n",
      "1595 1 664 931 tensor(2) 1001 5 3 4\n",
      "1599 1 668 931 tensor(2) 1001 5 3 4\n",
      "1603 1 672 931 tensor(2) 1001 5 3 4\n",
      "1607 1 676 931 tensor(2) 1001 5 3 4\n",
      "1611 1 680 931 tensor(2) 1001 5 3 4\n",
      "1615 1 684 931 tensor(2) 1001 5 3 4\n",
      "1619 1 688 931 tensor(2) 1001 5 3 4\n",
      "1623 1 692 931 tensor(2) 1001 5 3 4\n",
      "1627 1 696 931 tensor(2) 1001 5 3 4\n",
      "1631 1 700 931 tensor(2) 1001 5 3 4\n",
      "1635 1 704 931 tensor(2) 1001 5 3 4\n",
      "1639 1 708 931 tensor(2) 1001 5 3 4\n",
      "1643 1 712 931 tensor(2) 1001 5 3 4\n",
      "1647 1 716 931 tensor(2) 1001 5 3 4\n",
      "1651 1 720 931 tensor(2) 1001 5 3 4\n",
      "1655 1 724 931 tensor(2) 1001 5 3 4\n",
      "1659 1 728 931 tensor(2) 1001 5 3 4\n",
      "1663 1 732 931 tensor(2) 1001 5 3 4\n",
      "1667 1 736 931 tensor(2) 1001 5 3 4\n",
      "1671 1 740 931 tensor(2) 1001 5 3 4\n",
      "1675 1 744 931 tensor(2) 1001 5 3 4\n",
      "1679 1 748 931 tensor(2) 1001 5 3 4\n",
      "1683 1 752 931 tensor(2) 1001 5 3 4\n",
      "1687 1 756 931 tensor(2) 1001 5 3 4\n",
      "1691 1 760 931 tensor(2) 1001 5 3 4\n",
      "1695 1 764 931 tensor(2) 1001 5 3 4\n",
      "1699 1 768 931 tensor(2) 1001 5 3 4\n",
      "1703 1 772 931 tensor(2) 1001 5 3 4\n",
      "1707 1 776 931 tensor(2) 1001 5 3 4\n",
      "1711 1 780 931 tensor(2) 1001 5 3 4\n",
      "1715 1 784 931 tensor(2) 1001 5 3 4\n",
      "1719 1 788 931 tensor(2) 1001 5 3 4\n",
      "1723 1 792 931 tensor(2) 1001 5 3 4\n",
      "1727 1 796 931 tensor(2) 1001 5 3 4\n",
      "1731 1 800 931 tensor(2) 1001 5 3 4\n",
      "1735 1 804 931 tensor(2) 1001 5 3 4\n",
      "1739 1 808 931 tensor(2) 1001 5 3 4\n",
      "1743 1 812 931 tensor(2) 1001 5 3 4\n",
      "1747 1 816 931 tensor(2) 1001 5 3 4\n",
      "1751 1 820 931 tensor(2) 1001 5 3 4\n",
      "1755 1 824 931 tensor(2) 1001 5 3 4\n",
      "1759 1 828 931 tensor(2) 1001 5 3 4\n",
      "1763 1 832 931 tensor(2) 1001 5 3 4\n",
      "1767 1 836 931 tensor(2) 1001 5 3 4\n",
      "1771 1 840 931 tensor(2) 1001 5 3 4\n",
      "1775 1 844 931 tensor(2) 1001 5 3 4\n",
      "1779 1 848 931 tensor(2) 1001 5 3 4\n",
      "1783 1 852 931 tensor(2) 1001 5 3 4\n",
      "1787 1 856 931 tensor(2) 1001 5 3 4\n",
      "1791 1 860 931 tensor(2) 1001 5 3 4\n",
      "1795 1 864 931 tensor(2) 1001 5 3 4\n",
      "1799 1 868 931 tensor(2) 1001 5 3 4\n",
      "1803 1 872 931 tensor(2) 1001 5 3 4\n",
      "1807 1 876 931 tensor(2) 1001 5 3 4\n",
      "1811 1 880 931 tensor(2) 1001 5 3 4\n",
      "1815 1 884 931 tensor(2) 1001 5 3 4\n",
      "1819 1 888 931 tensor(2) 1001 5 3 4\n",
      "1823 1 892 931 tensor(2) 1001 5 3 4\n",
      "1827 1 896 931 tensor(2) 1001 5 3 4\n",
      "1831 1 900 931 tensor(2) 1001 5 3 4\n",
      "1835 1 904 931 tensor(2) 1001 5 3 4\n",
      "1839 1 908 931 tensor(2) 1001 5 3 4\n",
      "1843 1 912 931 tensor(2) 1001 5 3 4\n",
      "1847 1 916 931 tensor(2) 1001 5 3 4\n",
      "1851 1 920 931 tensor(2) 1001 5 3 4\n",
      "1855 1 924 931 tensor(2) 1001 5 3 4\n",
      "1859 1 928 931 tensor(2) 1001 5 3 4\n"
     ]
    }
   ],
   "source": [
    "for idx, (front, label) in enumerate(loader1):\n",
    "    #print(idx, front.shape, label.shape)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81faab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import math\n",
    "from modelComp.utils import ConvNeXtBlock, ResNetBlock, SwiGLU, MLP\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim = 96, data_dim = (1,5,4,128,128), patch_size = (8,8), hiddenout_dim = 256, act=nn.GELU):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.B, self.T, self.C, self.H, self.W = data_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.pH, self.pW = patch_size\n",
    "        self.hiddenout_dim = hiddenout_dim\n",
    "        self.patch_grid_res = (self.H // self.pH, self.W // self.pW)\n",
    "        \n",
    "\n",
    "        assert self.H % self.pH == 0 and self.W % self.pW == 0, \"spatial input dim must be divisible by patch_size\"\n",
    "        assert self.H == self.W, \"must be square\"\n",
    "        \n",
    "\n",
    "        self.patchify = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.unpatchify = nn.Fold(output_size=(self.H, self.W), kernel_size=patch_size, stride=patch_size)\n",
    "        self.pre_proj = nn.Sequential(\n",
    "            nn.Linear(self.C * self.pH * self.pW, self.emb_dim * 2 if act == SwiGLU else self.emb_dim),\n",
    "            act(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim),\n",
    "        )\n",
    "\n",
    "        self.post_proj = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, self.hiddenout_dim* 2 if act == SwiGLU else self.hiddenout_dim),\n",
    "            act(),\n",
    "            nn.Linear(self.hiddenout_dim, self.hiddenout_dim* 2 if act == SwiGLU else self.hiddenout_dim),\n",
    "            act(),\n",
    "            nn.Linear(self.hiddenout_dim, self.C * self.pH * self.pW),\n",
    "        )\n",
    "\n",
    "    #def get_pos_embeddings(self, t_len):\n",
    "    #    return (self.time_embed[:, :t_len] + self.patch_position_embeddings).view(1, -1, self.emb_dim)  # (1, t*p*p, d)\n",
    "\n",
    "    def encode(self, x, proj=True):\n",
    "\n",
    "        B, T, C, H, W = x.shape\n",
    "        #print(1, x.shape)\n",
    "        #x = rearrange(x, \"b t c h w -> (b t) c h w\") #might change to .permute\n",
    "        #print(2, x.shape)\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        #print('before patchify', x.shape)\n",
    "        x = self.patchify(x)  \n",
    "        #print('after patchify', x.shape)\n",
    "        #print(3, x.shape)\n",
    "        x = rearrange(x, '(b t) d n -> b t n d', b=B, t=T)\n",
    "        #print('after rearrange', x.shape)\n",
    "        #x = rearrange(x, \"(b t) d pp -> b (t pp) d\", b=B) # should think about this\n",
    "        #x = rearrange(x, \"b d pp -> b pp d\")\n",
    "        #print(4, x.shape)\n",
    "\n",
    "        # TODO: add Positional Encoding\n",
    "        if proj:\n",
    "            #print(5, x.shape)\n",
    "            return self.pre_proj(x)#.transpose(1, 2)\n",
    "        else:\n",
    "            return x#.transpose(1, 2)\n",
    "\n",
    "    def decode(self, x, proj=True):\n",
    "        if proj:\n",
    "            x = self.post_proj(x)  \n",
    "\n",
    "        B, T, N, D = x.shape\n",
    "        #x = rearrange(x, \"b (t pp) d -> (b t) d pp\", pp=self.patch_grid_res[0]*self.patch_grid_res[1]) #might change to .permute\n",
    "        x = rearrange(x, 'b t n d -> (b t) d n')\n",
    "        #print('before unpatchify decode', x.shape)\n",
    "        x = self.unpatchify(x)  \n",
    "        #print(x.shape)\n",
    "        #x = rearrange(x, \"(b t) c h w -> b t c h w\", b=B)\n",
    "        x = rearrange(x, '(b t) c h w -> b t c h w', t=T)\n",
    "        #print('end decoder: ', x.shape)\n",
    "        return x\n",
    "\n",
    "class SpatiotemporalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, height, width, timesteps):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.T = timesteps\n",
    "\n",
    "        assert emb_dim % 2 == 0, \"emb_dim must be divisible by 2\"\n",
    "        self.row_embed = nn.Parameter(torch.randn(1, self.H, 1, emb_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.randn(1, 1, self.W, emb_dim // 2))\n",
    "        self.time_embed = nn.Parameter(torch.randn(1, self.T, 1, emb_dim))  # full D for time\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, N, D)\n",
    "        B, T, N, D = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert N == H * W, f\"Expected N = H*W = {H}x{W}, got {N}\"\n",
    "\n",
    "        # Spatial embeddings: (1, H, W, D)\n",
    "        spatial = torch.cat([\n",
    "            self.row_embed.expand(-1, H, W, -1),  # (1, H, W, D//2)\n",
    "            self.col_embed.expand(-1, H, W, -1)   # (1, H, W, D//2)\n",
    "        ], dim=-1).view(1, 1, N, D)  # -> (1, 1, N, D)\n",
    "\n",
    "        # Temporal embedding: (1, T, 1, D)\n",
    "        time = self.time_embed  # already (1, T, 1, D)\n",
    "\n",
    "        pos = spatial + time  # broadcasts over (B, T, N, D)\n",
    "\n",
    "        return x + pos\n",
    " \n",
    "class PatchMerge(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4*emb_dim, 2*emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, L, C = x.shape\n",
    "        H = W = int(np.sqrt(L)/2)\n",
    "        x = rearrange(x, 'b t (h s1 w s2) c -> b t (h w) (s1 s2 c)', s1=2, s2=2, h=H, w=W) #might change to .permute\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class PatchUnMerge(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(emb_dim, 2*emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, L, C = x.shape\n",
    "        H = W = int(np.sqrt(L))\n",
    "        #print(x.shape, H, W)\n",
    "        x = self.linear(x)\n",
    "        #print(x.shape)\n",
    "        x = rearrange(x, 'b t (h w) (s1 s2 c) -> b t (h s1 w s2) c', s1=2, s2=2, h=H, w=W) #might change to .permute\n",
    "        return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n",
    "    use_flex_attn=True, act=nn.ReLU):\n",
    "\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.use_flex_attn = use_flex_attn # original from logits scale BCAT?\n",
    "\n",
    "        assert emb_dim % num_heads == 0, \"embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "\n",
    "        if self.use_flex_attn: \n",
    "            self.flex_attn = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
    "            #self.register_parameter(\"flex_attn\", nn.Parameter(self.flex_attn))\n",
    "\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n",
    "                                     act(),\n",
    "                                     nn.Linear(512, num_heads, bias=False))\n",
    "\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(\n",
    "            torch.meshgrid([relative_coords_h,\n",
    "                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  \n",
    "        \n",
    "        relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n",
    "        relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n",
    "        relative_coords_table *= 8  # normalize to -8, 8 TODO: understand why\n",
    "        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n",
    "            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w])) \n",
    "        coords_flatten = torch.flatten(coords, 1)  \n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  \n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  \n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  \n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  \n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(emb_dim, emb_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(emb_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(emb_dim))\n",
    "            # TODO: understand why not key bias\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_out = nn.Linear(emb_dim, emb_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        #print(x.shape, self.qkv.weight.shape, qkv_bias.shape)\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        #print('test')\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # cosine attention\n",
    "        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n",
    "        if self.use_flex_attn:\n",
    "            flex_attn = torch.clamp(self.flex_attn, max=torch.log(torch.tensor(1. / 0.01, device=x.device))).exp()\n",
    "            attn = attn * flex_attn\n",
    "\n",
    "        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n",
    "        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) \n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  \n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj_out(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SpatialSwinBlock(nn.Module): #change name to something else\n",
    "\n",
    "    def __init__(self, emb_dim, patch_grid_res, num_heads, window_size=4, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., use_flex_attn=True, \n",
    "                 act=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_grid_res = patch_grid_res\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        #self.use_proj_in = use_proj_in\n",
    "        #print('shift_size', self.shift_size)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(emb_dim)\n",
    "        #print('test')\n",
    "        self.attn = WindowAttention(\n",
    "            emb_dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, use_flex_attn=use_flex_attn,\n",
    "            act=nn.ReLU) # act is for relative position bias mlp so maybe not use swiglu/gelu\n",
    "        #print('test')\n",
    "        self.norm2 = norm_layer(emb_dim)\n",
    "        mlp_hidden_dim = int(emb_dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=emb_dim, hidden_features=mlp_hidden_dim, act=act, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA (from original swin paper source code)\n",
    "            H, W = self.patch_grid_res\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            #print('test')\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.patch_grid_res\n",
    "        B, T, L, C = x.shape\n",
    "        assert L == H * W, f\"Expected L = H*W = {H}x{W}, but got L = {L}\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = x.view(B * T, H, W, C)  # Combine B and T for window partitioning\n",
    "\n",
    "        # Apply cyclic shift if needed\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "\n",
    "        # Partition into windows and compute attention\n",
    "        x_windows = window_partition(x, self.window_size)  # (num_windows*B*T, ws, ws, C)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # (num_windows*B*T, ws*ws, C)\n",
    "\n",
    "        # Merge windows back\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W)  # (B*T, H, W, C)\n",
    "\n",
    "        # Reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "\n",
    "        # Reshape back to (B, T, H*W, C)\n",
    "        x = x.view(B, T, H * W, C)\n",
    "\n",
    "        # Residual + MLP\n",
    "        x = shortcut + self.norm1(x)\n",
    "        x = x + self.norm2(self.mlp(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, max_timesteps=5, mlp_ratio=4.0,\n",
    "                 qkv_bias=True, drop=0.0, attn_drop=0.0, use_flex_attn=True,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_timesteps = max_timesteps\n",
    "\n",
    "        self.norm1 = norm_layer(emb_dim)\n",
    "        self.qkv = nn.Linear(emb_dim, emb_dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "\n",
    "        self.use_flex_attn = use_flex_attn\n",
    "        if self.use_flex_attn:\n",
    "            self.flex_attn = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
    "\n",
    "        # Continuous relative positional bias (not trainable)\n",
    "        relative_positions = torch.arange(-max_timesteps + 1, max_timesteps, dtype=torch.float32)\n",
    "        relative_positions /= (max_timesteps - 1)\n",
    "        bias_values = torch.sign(relative_positions) * torch.log2(torch.abs(relative_positions) + 1.0) / np.log2(2)\n",
    "        bias_table = bias_values.unsqueeze(1).repeat(1, num_heads)\n",
    "        self.register_buffer(\"relative_position_bias_table\", bias_table)\n",
    "\n",
    "        coords = torch.arange(max_timesteps)\n",
    "        relative_coords = coords[None, :] - coords[:, None]\n",
    "        relative_coords += max_timesteps - 1\n",
    "        self.register_buffer(\"relative_position_index\", relative_coords)\n",
    "\n",
    "        self.norm2 = norm_layer(emb_dim)\n",
    "        mlp_hidden_dim = int(emb_dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=emb_dim, hidden_features=mlp_hidden_dim,\n",
    "                       act=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, E, C)\n",
    "        B, T, E, C = x.shape\n",
    "        assert T <= self.max_timesteps, f\"Input timesteps {T} exceed max_timesteps {self.max_timesteps}\"\n",
    "\n",
    "        # ============ Attention Block ============\n",
    "        x_ = x.permute(0, 2, 1, 3).contiguous()  # (B, E, T, C)\n",
    "        x_ = self.norm1(x_).reshape(B * E, T, C)\n",
    "\n",
    "        qkv = self.qkv(x_).reshape(B * E, T, 3, self.num_heads, C // self.num_heads)\n",
    "        q, k, v = qkv.unbind(dim=2)  # (B*E, T, num_heads, head_dim)\n",
    "        q = q.transpose(1, 2)  # (B*E, heads, T, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (C // self.num_heads) ** 0.5\n",
    "        if self.use_flex_attn:\n",
    "            max_log = torch.log(torch.tensor(1. / 0.01, device=attn.device))\n",
    "            flex_attn = torch.clamp(self.flex_attn, max=max_log).exp()\n",
    "            attn = attn * flex_attn\n",
    "\n",
    "        bias = self.relative_position_bias_table[self.relative_position_index[:T, :T].reshape(-1)]\n",
    "        bias = bias.view(T, T, self.num_heads).permute(2, 0, 1)  # (heads, T, T)\n",
    "        attn = attn + bias.unsqueeze(0)\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        attn_out = (attn @ v).transpose(1, 2).reshape(B * E, T, C)\n",
    "        attn_out = self.proj(attn_out)\n",
    "        attn_out = self.proj_drop(attn_out)\n",
    "\n",
    "        x_ = x_ + attn_out  # Residual 1\n",
    "        x_ = x_.reshape(B, E, T, C).permute(0, 2, 1, 3).contiguous()  # (B, T, E, C)\n",
    "\n",
    "        x_norm = self.norm2(x_)\n",
    "        x = x_ + self.mlp(x_norm)  # Residual 2\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SwinStage(nn.Module): # change name since stage also includes patch merge formally\n",
    "    \n",
    "    def __init__(self, emb_dim, patch_grid_res, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., use_flex_attn=True,\n",
    "                 act=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.patch_grid_res = patch_grid_res\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SpatialSwinBlock(emb_dim=emb_dim, patch_grid_res=patch_grid_res,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, \n",
    "                                 drop=drop,\n",
    "                                 attn_drop=attn_drop,\n",
    "                                 use_flex_attn=use_flex_attn,\n",
    "                                 act=act,\n",
    "                                 norm_layer=norm_layer\n",
    "                                 )\n",
    "            for i in range(2)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return x\n",
    "\n",
    "class SwinUnet(nn.Module):\n",
    "    def __init__(self, emb_dim, data_dim, patch_size, hiddenout_dim, depth, \n",
    "                 stage_depths, num_heads, window_size=8, mlp_ratio=4., \n",
    "                 qkv_bias=True, drop=0., attn_drop=0., use_flex_attn=True, norm_layer=nn.LayerNorm,\n",
    "                 act=nn.GELU, skip_connect=ConvNeXtBlock, gradient_flowthrough=[True, False, False]):\n",
    "        super().__init__()\n",
    "\n",
    "        # assert that every element in stage_depths is divisible by 3 except for the middle element\n",
    "        assert all(stage_depths[i] % 3 == 0 for i in range(len(stage_depths)) if i != depth), \"stage depth must be divisible by 3 at non-middle elements\"\n",
    "        assert stage_depths[depth] % 2 == 0, \"stage depth must be divisible by 2 at middle element\"\n",
    "        self.embedding = LinearEmbedding(emb_dim, data_dim, patch_size, hiddenout_dim, act)\n",
    "        self.pos_encoding = SpatiotemporalPositionalEncoding(emb_dim, data_dim[3] // patch_size[0], data_dim[4] // patch_size[1], data_dim[1])\n",
    "        #print(data_dim[2] // patch_size[0], data_dim[2] // patch_size[0], data_dim[1])\n",
    "        self.blockDown = [nn.ModuleList() for i in range(depth)]\n",
    "        self.blockMiddle = nn.ModuleList()\n",
    "        self.blockUp = [nn.ModuleList() for i in range(depth)]\n",
    "        self.patchMerges = nn.ModuleList()\n",
    "        self.patchUnmerges = nn.ModuleList()\n",
    "        self.skip_connects = nn.ModuleList()\n",
    "        # TODO: implement act \n",
    "\n",
    "        self.depth = depth\n",
    "        self.middleblocklen = stage_depths[depth]\n",
    "        self.gradient_flowthrough = gradient_flowthrough\n",
    "        self.skip_connect = skip_connect\n",
    "\n",
    "        for i in range(depth):\n",
    "            patch_grid_res = (data_dim[3] // (patch_size[0] * 2**i), data_dim[4] // (patch_size[1] * 2**i))\n",
    "            for j in range(stage_depths[i]):\n",
    "                #print(j)\n",
    "                if j % 3 == 0:\n",
    "                    self.blockDown[i].append(\n",
    "                        SwinStage(\n",
    "                            emb_dim * 2**i, \n",
    "                            patch_grid_res=patch_grid_res, \n",
    "                            num_heads=num_heads[i], \n",
    "                            window_size=window_size, \n",
    "                            mlp_ratio = mlp_ratio, \n",
    "                            qkv_bias = qkv_bias, \n",
    "                            drop=drop,\n",
    "                            attn_drop = attn_drop,\n",
    "                            use_flex_attn = use_flex_attn, \n",
    "                            act=act, \n",
    "                            norm_layer=norm_layer\n",
    "                        )\n",
    "                    )\n",
    "                    j += 1\n",
    "                elif j % 3 == 2:\n",
    "                    self.blockDown[i].append(\n",
    "                        TemporalBlock(\n",
    "                            emb_dim=emb_dim * 2**i,\n",
    "                            num_heads=num_heads[i],\n",
    "                            max_timesteps=data_dim[1],\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            drop=drop,\n",
    "                            attn_drop=attn_drop,\n",
    "                            use_flex_attn=use_flex_attn,\n",
    "                            act_layer=act,\n",
    "                            norm_layer=norm_layer\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            self.patchMerges.append(PatchMerge(emb_dim * 2**i))\n",
    "\n",
    "\n",
    "        patch_grid_res = (data_dim[3] // (patch_size[0] * 2**depth), data_dim[4] // (patch_size[1] * 2**depth))\n",
    "        full_window_size = data_dim[3] // (patch_size[0] * 2**depth)\n",
    "        #print('full_window_size', full_window_size)\n",
    "        for i in range(stage_depths[depth]):\n",
    "            if i % 2 == 0:\n",
    "\n",
    "                self.blockMiddle.append(\n",
    "                    SpatialSwinBlock(\n",
    "                        emb_dim * 2**depth,\n",
    "                        patch_grid_res=patch_grid_res,\n",
    "                        num_heads=num_heads[depth],\n",
    "                        window_size = full_window_size,\n",
    "                        shift_size=0,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop=0.,\n",
    "                        attn_drop=0.,\n",
    "                        use_flex_attn=use_flex_attn,\n",
    "                        act=act,\n",
    "                        norm_layer=norm_layer\n",
    "                    )\n",
    "                )\n",
    "            if i % 2 == 1:\n",
    "                self.blockMiddle.append(\n",
    "                    TemporalBlock(\n",
    "                        emb_dim=emb_dim * 2**depth,\n",
    "                        num_heads=num_heads[depth],\n",
    "                        max_timesteps=data_dim[1],\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop=drop,\n",
    "                        attn_drop=attn_drop,\n",
    "                        use_flex_attn=use_flex_attn,\n",
    "                        act_layer=act,\n",
    "                        norm_layer=norm_layer\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for i in reversed(range(depth)):\n",
    "            #print(i)\n",
    "            patch_grid_res = (data_dim[3] // (patch_size[0] * 2**i), data_dim[4] // (patch_size[1] * 2**i))\n",
    "            for j in range(stage_depths[depth + i + 1]):\n",
    "                #print(depth + i, j)\n",
    "                #print(i, emb_dim * 2**i)\n",
    "                if j % 3 == 0:\n",
    "                    self.blockUp[depth - i - 1].append(\n",
    "                        SwinStage(\n",
    "                            emb_dim * 2**i, \n",
    "                            patch_grid_res=patch_grid_res, \n",
    "                            num_heads=num_heads[2*depth - i], \n",
    "                            window_size=window_size, \n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            drop=drop,\n",
    "                            attn_drop=attn_drop,\n",
    "                            use_flex_attn=use_flex_attn,\n",
    "                            act=act,\n",
    "                            norm_layer=norm_layer\n",
    "                        )\n",
    "                    )\n",
    "                    j += 1\n",
    "                elif j % 3 == 2:\n",
    "                    self.blockUp[depth - i - 1].append(\n",
    "                        TemporalBlock(\n",
    "                            emb_dim=emb_dim * 2**i,\n",
    "                            num_heads=num_heads[2*depth - i],\n",
    "                            max_timesteps=data_dim[1],\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            drop=drop,\n",
    "                            attn_drop=attn_drop,\n",
    "                            use_flex_attn=use_flex_attn,\n",
    "                            act_layer=act,\n",
    "                            norm_layer=norm_layer\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "            self.patchUnmerges.append(PatchUnMerge(emb_dim * 2**(i+1)))\n",
    "            self.skip_connects.append(skip_connect(emb_dim * 2**i)) if skip_connect is not None else None\n",
    "            #print(len(self.blockUp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape checks\n",
    "        if x.ndim != 5:\n",
    "            raise ValueError(f\"Input tensor must be 5D, but got {x.ndim}D\")\n",
    "        if x.shape[1] != self.embedding.T or x.shape[2] != self.embedding.C or x.shape[3] != self.embedding.H or x.shape[4] != self.embedding.W:\n",
    "            raise ValueError(f\"Input tensor must be of shape (B, {self.embedding.T}, {self.embedding.C}, {self.embedding.H}, {self.embedding.W}), but got {x.shape}\")\n",
    "        skips = []\n",
    "        #print('module_list', self.blockDown)\n",
    "        #print('block up', self.blockUp)\n",
    "        x = self.embedding.encode(x, proj=True)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # ===== DOWN =====\n",
    "        for i, module_list in enumerate(self.blockDown):\n",
    "            \n",
    "            if self.gradient_flowthrough[0]:\n",
    "                residual = x\n",
    "                for module in module_list:\n",
    "                    x = module(x)\n",
    "                skips.append(x)\n",
    "                x = x + residual\n",
    "            else:\n",
    "                for module in module_list:\n",
    "                    x = module(x)\n",
    "                skips.append(x)\n",
    "\n",
    "            x = self.patchMerges[i](x)\n",
    "\n",
    "        # ===== MIDDLE =====\n",
    "        residual = x\n",
    "        for module in self.blockMiddle:\n",
    "            #print(module)\n",
    "            x = module(x)\n",
    "\n",
    "        if self.gradient_flowthrough[1]:\n",
    "            x = x + residual\n",
    "\n",
    "        # ===== UP =====\n",
    "        for i, module_list in enumerate(self.blockUp):\n",
    "            x = self.patchUnmerges[i](x)\n",
    "            #x = x + self.skip_connects[i](skips[self.depth - i - 1])\n",
    "            skip = skips[self.depth - i - 1]\n",
    "            x = x + (self.skip_connects[i](skip) if self.skip_connect is not None else skip)\n",
    "            if self.gradient_flowthrough[2]:\n",
    "                residual = x\n",
    "                for module in module_list:\n",
    "                    x = module(x)\n",
    "                x = x + residual\n",
    "            else:\n",
    "                for module in module_list:\n",
    "                    #print(module)\n",
    "                    x = module(x)\n",
    "\n",
    "\n",
    "        x = self.embedding.decode(x, proj=True)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6889664b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x = \u001b[43mtorch\u001b[49m.randn(\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)\n\u001b[32m      2\u001b[39m embedding = LinearEmbedding(\u001b[32m96\u001b[39m, x.shape, (\u001b[32m4\u001b[39m,\u001b[32m4\u001b[39m), \u001b[32m256\u001b[39m)\n\u001b[32m      3\u001b[39m x = embedding.encode(x)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 5, 2, 128, 128)\n",
    "embedding = LinearEmbedding(96, x.shape, (4,4), 256)\n",
    "x = embedding.encode(x)\n",
    "#print('x', x.shape)\n",
    "pe = SpatiotemporalPositionalEncoding(96, int(x.shape[2]**(1/2)), int(x.shape[2]**(1/2)), x.shape[1])\n",
    "x = x + pe(x)\n",
    "#print('x', x.shape)\n",
    "stage1 = SwinStage(96, (32, 32), 2, 4, 4, 4, 0.1, 0.1, True)\n",
    "x = stage1(x)\n",
    "#print('x', x.shape)\n",
    "merge = PatchMerge(96)\n",
    "x = merge(x)\n",
    "#print('x', x.shape)\n",
    "middle = SpatialSwinBlock(192, (16, 16), 4, 4, 0, 4, True)\n",
    "x = middle(x)\n",
    "#print('x', x.shape)\n",
    "middle2 = TemporalBlock(192, 4, 5, 4, True, 0.1, 0.1)\n",
    "x = middle2(x)\n",
    "\n",
    "unmerge = PatchUnMerge(192)\n",
    "x = unmerge(x)\n",
    "#print('x', x.shape)\n",
    "stage2 = SwinStage(96, (32, 32), 2, 4, 4, 4, 0.1, 0.1, True)\n",
    "x = stage2(x)\n",
    "#print('x', x.shape)\n",
    "x = embedding.decode(x)\n",
    "#print('x', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6077ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y torch.Size([8, 4, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model = SwinUnet(96, data_dim=(8,4,2,256,256), patch_size=(4,4), \n",
    "                 hiddenout_dim=256, depth=2, stage_depths=[3,6,6,6,3], \n",
    "                 num_heads=[3,6,12,6,3], skip_connect=ConvNeXtBlock)\n",
    "x = torch.randn(8, 4, 2, 256, 256)\n",
    "y = model(x)\n",
    "print('y', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "578a0c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [3,6,4,6,3]\n",
    "all(x[i] % 3 == 0 for i in range(len(x)) if i != 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
