{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 'temperature'\n",
    "VELX = 'velx'\n",
    "VELY = 'vely'\n",
    "PRESSURE = 'pressure'\n",
    "\n",
    "\n",
    "class SimpleLoaderBubbleML(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = h5py.File(self.filename, 'r')\n",
    "        self.timesteps = self.data[TEMPERATURE][:].shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.timesteps - 1\n",
    "\n",
    "    def _get_state(self, idx):\n",
    "        r\"\"\"\n",
    "        The input is the temperature, x-velocity, and y-velocity at time == idx\n",
    "        \"\"\"\n",
    "        temp = torch.from_numpy(self.data[TEMPERATURE][idx])\n",
    "        velx = torch.from_numpy(self.data[VELX][idx])\n",
    "        vely = torch.from_numpy(self.data[VELY][idx])\n",
    "        # returns a stack with shape [3 x Y x X]\n",
    "        return torch.stack((temp, velx, vely), dim=0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\"\n",
    "        As input, get temperature and velocities at time == idx.\n",
    "        As the output label, get the temperature at time == idx + 1.\n",
    "        \"\"\"\n",
    "        input = self._get_state(idx)\n",
    "        label = self._get_state(idx+1)\n",
    "        return input, label\n",
    "    \n",
    "    def get_full_stack(self):\n",
    "        r\"\"\"\n",
    "        Retrieve the full temporal domain stack for the entire simulation.\n",
    "        The returned stack will have shape: [timesteps, channels, Y, X].\n",
    "        \"\"\"\n",
    "        # Load temperature, velx, and vely data from the HDF5 file\n",
    "        temp_data = torch.from_numpy(self.data[TEMPERATURE][:])  # Shape: [timesteps, Y, X]\n",
    "        velx_data = torch.from_numpy(self.data[VELX][:])         # Shape: [timesteps, Y, X]\n",
    "        vely_data = torch.from_numpy(self.data[VELY][:])         # Shape: [timesteps, Y, X]\n",
    "        \n",
    "        # Stack the data along the channel dimension\n",
    "        full_stack = torch.stack((temp_data, velx_data, vely_data), dim=1)  # Shape: [timesteps, channels, Y, X]\n",
    "        return full_stack\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../../data/redimensionalized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m downsampled\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Process each file in the directory\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     60\u001b[0m     input_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../../data/redimensionalized'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "redim = True\n",
    "\n",
    "# Parameters\n",
    "data_path = '../../data'\n",
    "\n",
    "downsample_factor = 4  # Downsample factor for spatial dimensions\n",
    "if redim:\n",
    "    output_path = os.path.join(data_path, 'downsampled_redimensionalized/')\n",
    "else:\n",
    "    output_path = os.path.join(data_path, 'downsampled/')\n",
    "\n",
    "data_path = data_path + '/redimensionalized' if redim else data_path\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)  # Ensure output directory exists\n",
    "\n",
    "keys_to_downsample = [\n",
    "    'temperature',\n",
    "    'velx',\n",
    "    'vely',\n",
    "    'dfun',\n",
    "    'pressure',\n",
    "    'x',\n",
    "    'y'\n",
    "]\n",
    "\n",
    "keys_to_copy = [\n",
    "    'real-runtime-params',\n",
    "    'int-runtime-params'\n",
    "]\n",
    "\n",
    "\n",
    "def downsample(data, factor):\n",
    "    \"\"\"\n",
    "    Downsample a tensor's spatial dimensions by the given factor.\n",
    "    Args:\n",
    "        data (torch.Tensor): 3D or 4D tensor [timesteps, channels, Y, X].\n",
    "        factor (int): Downsampling factor for spatial dimensions.\n",
    "    Returns:\n",
    "        torch.Tensor: Downsampled tensor.\n",
    "    \"\"\"\n",
    "    if factor == 1:  # No downsampling needed\n",
    "        return data\n",
    "    _, _, height, width = data.shape\n",
    "    new_height, new_width = height // factor, width // factor\n",
    "\n",
    "    # Use PyTorch's interpolate for downsampling\n",
    "    downsampled = F.interpolate(data, size=(new_height, new_width), mode='area')\n",
    "    return downsampled\n",
    "\n",
    "\n",
    "# Process each file in the directory\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.hdf5')]\n",
    "\n",
    "for file in files:\n",
    "    input_file = os.path.join(data_path, file)\n",
    "    output_file = os.path.join(output_path, file)\n",
    "\n",
    "    with h5py.File(input_file, 'r') as input_data:\n",
    "        # Prepare output HDF5 file\n",
    "        with h5py.File(output_file, 'w') as output_data:\n",
    "            for key in input_data.keys():\n",
    "                dataset = input_data[key][:]\n",
    "                print(f\"Processing key '{key}' in file '{file}' with shape {dataset.shape}\")\n",
    "\n",
    "                if key in keys_to_downsample:\n",
    "                    # Downsample spatial-temporal data\n",
    "                    if len(dataset.shape) >= 3:  # Process only 3D or 4D data\n",
    "                        if len(dataset.shape) == 3:\n",
    "                            dataset = torch.from_numpy(dataset).unsqueeze(1)  # Add channel dimension\n",
    "                        else:\n",
    "                            dataset = torch.from_numpy(dataset)\n",
    "\n",
    "                        downsampled = downsample(dataset, downsample_factor)\n",
    "                        downsampled = downsampled.numpy()\n",
    "                        # Remove channel dimension for datasets that were originally 3D\n",
    "                        if downsampled.shape[1] == 1:\n",
    "                            downsampled = downsampled[:, 0]\n",
    "                    else:\n",
    "                        raise ValueError(f\"Key '{key}' expected to have spatial dimensions but doesn't.\")\n",
    "\n",
    "                    # Save downsampled \n",
    "                    if key == 'temperature':\n",
    "                        print('max termperature:', np.max(downsampled))\n",
    "                    output_data.create_dataset(key, data=downsampled)\n",
    "\n",
    "                elif key in keys_to_copy:\n",
    "                    # Directly copy keys\n",
    "                    output_data.create_dataset(key, data=dataset)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Skipping key '{key}' as it's not in keys_to_downsample or keys_to_copy.\")\n",
    "\n",
    "            print(f\"File '{file}' processed and saved to '{output_file}'.\")\n",
    "\n",
    "print(\"Downsampling completed for all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
